\section{Introduction}

The tasks of object recognition and spatial localization require the ability to find similarities in sensor readings. There are two main approaches for this task: the global and the local one. The essence of the global approach is an attempt to treat the sensor scan as a whole. Well-known iterated closest point method \cite{besl1992method} can serve as a typical example of a global method. Local methods approach the problem differently: the attention is paid to remarkable parts of sensor readings. Such methods typically include two separate steps, of which the first is to find important places in the scan and the second is to produce a suitable for further work local descriptor  for the interesting part. The notion of important part of the scan is often simplified to the neighborhood of some point, which in this case is called \textit{feature point} or \textit{key point}. The first name takes it origin from   
the understanding of the extracted descriptors as scan \textit{features}, which is a typical name for the input to higher level algorithms of pattern recognition and machine learning. Feature points search algorithms should satisfy a number of requirements, of which translation and rotation invariances are of primary importance. Another desirable property for these algorithms is scale invariance.

The most wide-spread type of sensor data are 2D images, since they are as long as several decades very easy and cheap to obtain. A lot of research has been done in the field of computer vision addressing the problem of feature points search for images. The famous examples include Harris and Stephens corner detector \cite{harris1988combined}, Lagrangian of a Gaussian (LoG) approach \cite{lindeberg1998feature}, popular Scale Invariant Feature Transform (SIFT) \cite{lowe2004distinctive} and faster SURF and FAST methods \cite{bay2006surf}, \cite{rosten2010faster}.

Another type of sensor data are 3D scans obtained by various depth-measuring devices. Lots of approaches for feature point search for 3D data include adaptations of classical computer vision methods like in \cite{sipiran2011harris} and novel methods, specially developed for this type of data, like Intrinsic Shape Signatures \cite{zhong2009intrinsic} and NARF \cite{steder2011point}. The last is highly heuristic, designed in the context of robotics mainly for organized point clouds, such as range images obtained by cheap Kinect-like sensors.

In this report an attempt to develop new feature point search algorithm for range images is documented. A recently proposed method for calculating shape representation, called Support Vector Shapes, served as the source of inspiration. In the corresponding paper  \cite{SVS2013} the authors consider shape representation and a classification task and use RBF-kernel Support Vector Machine (SVM) \cite{cortes1995support} as a classifier. They used typical two-stage scheme as outlined above with feature points and local descriptors and were able to achieve close to state-of-the-art result for shape retrieval on MPEG-7 data set \cite{latecki2000shape}. 

The adaptation of Support Vector Shapes from 2D contour to 3D shapes is not an easy task. The primary challenge is speed, since   
SVM is not a linear algorithm and range images are quite voluminous. A successful attempt to speed up SVM for this particular  task is described, which made it possible to evaluate stability of resulting keypoints and compare it with one of NARF.
