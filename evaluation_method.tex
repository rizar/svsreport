To evaluate the keypoint extraction algorithm I used RGB-D dataset from \cite{endres2012evaluation}. The dataset contains 39 sequences of color images and depth images from Kinect camera with resolution 640 $\times$ 480. The ground truth pose of the sensor from motion capture system is available for each sequence, that allows to calculate locations of all the extracted feature points in the world frame. I use the sequence ``fr1/xyz''. It was recorded in the typical office environment: there is a table in the center of the scene with various objects lying on it. The camera was held by a hand and performed isolated motions along the coordinate axes.

To run the keypoint extraction on the sequences of scans and deal with geometric transformations I used the popular middleware Robot Operating System (ROS) \cite{ROS2009}. Visualization was also done by means of ROS, specifically with RViz. The implementation from Point Cloud Library \cite{rusu20113d} was used to represent NARF keypoint extraction algorithm. It was used  with default parameters except the support size, which varied.